# -*- coding: utf-8 -*-
"""XLNetModelUsingV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IhZd3L3VHR_yfGAgk2sjtwphBhEc5XRn

Dyanamic
"""

import os
import re
from PyPDF2 import PdfReader
from transformers import XLNetModel, XLNetTokenizer
import torch
import numpy as np
import sentencepiece

def clean_function(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]', r' ', resumeText)
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText

def convert_pdf_to_text(pdf_path):
    with open(pdf_path, 'rb') as file:
        pdf_reader = PdfReader(file)
        text = ''
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()
        return text

def process_resumes(folder_path, job_description, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(folder_path, filename)
            text = convert_pdf_to_text(pdf_path)

            # Clean resume text
            text = clean_function(text)

            # Clean job description
            job_description_clean = clean_function(job_description)

            # Import necessary libraries
            model = XLNetModel.from_pretrained('xlnet-base-cased')
            tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')

            # Define the texts
            texts = [job_description_clean, text]

            # Compute embeddings for each text
            embeddings = []
            for text in texts:
                # Tokenize and encode the text
                input_ids = tokenizer.encode(text, add_special_tokens=True)

                # Convert input ids to tensor
                input_ids = torch.tensor([input_ids])

                # Compute embeddings
                with torch.no_grad():
                    last_hidden_states = model(input_ids)[0]

                # Compute sentence embeddings
                sentence_embedding = torch.mean(last_hidden_states, dim=1).numpy()

                # Add the embedding to the list
                embeddings.append(sentence_embedding)

            # Compute cosine similarity between the embeddings
            cosine_sim = np.dot(embeddings[0].flatten(), embeddings[1].flatten()) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))

            # Check similarity threshold (adjust as needed)
            if cosine_sim > 0.8:
                selected_resumes_path = os.path.join(output_folder, filename.replace(".pdf", "_selected.txt"))
                with open(selected_resumes_path, 'w', encoding='utf-8') as output_file:
                    output_file.write(text)

    print("Resumes processed and selected ones saved in:", output_folder)

# Specify the paths
resume_folder_path = "path/to/resumes/folder"
job_description_path = "path/to/job/description.txt"
output_folder_path = "path/to/output/folder"

# Read the job description text
with open(job_description_path, 'r', encoding='utf-8') as job_desc_file:
    job_description = job_desc_file.read()

# Process resumes
process_resumes(resume_folder_path, job_description, output_folder_path)